# üß† Chain-of-Thought (CoT)

## What It Is:

**Chain-of-Thought (CoT)** is a reasoning strategy where the LLM is prompted to break down a problem into **explicit intermediate steps**, rather than jumping to an answer. Think of it as teaching the model to ‚Äúthink aloud.‚Äù

This results in:

* More accurate answers
* Better interpretability
* Easier error diagnosis

CoT is a **prompt-only technique** ‚Äî no tools or APIs needed. It‚Äôs lightweight and highly effective across tasks.

---

## üß© Step-by-Step Breakdown

Here‚Äôs how CoT reasoning unfolds:

| Step                          | Function                             | Example Output                                                                 |
| ----------------------------- | ------------------------------------ | ------------------------------------------------------------------------------ |
| 1. **User Input**             | The user provides a question or task | ‚ÄúWhich country has the larger population: Australia or Germany?‚Äù               |
| 2. **Step-by-Step Thought**   | The model breaks it down logically   | ‚ÄúFirst, I‚Äôll recall population sizes. Australia has \~26M, Germany has \~83M‚Ä¶‚Äù |
| 3. **Intermediate Reasoning** | The model makes comparisons          | ‚ÄúGermany‚Äôs population is significantly higher.‚Äù                                |
| 4. **Final Answer**           | The model concludes explicitly       | ‚ÄúSo the answer is Germany.‚Äù                                                    |

---

## üß™ Examples in Action

### üîπ Example 1: Numerical Reasoning

**Prompt:**
*‚ÄúIf a train leaves the station at 3 PM and travels 80km at 40km/h, what time does it arrive?‚Äù*

**CoT Response:**

> 1. It‚Äôs going 80 km at 40 km/h.
> 2. Time = distance / speed = 80 / 40 = 2 hours.
> 3. 3 PM + 2 hours = 5 PM.
>    Final Answer: 5 PM

### üîπ Example 2: Social Science Coding

**Prompt:**
*‚ÄúDoes this quote support or oppose the policy change?‚Äù*

**Quote:**
*‚ÄúI think we‚Äôve had enough of this government meddling in local services.‚Äù*

**CoT Response:**

> 1. The speaker refers negatively to government involvement.
> 2. ‚ÄúHad enough‚Äù implies frustration or opposition.
> 3. ‚ÄúMeddling‚Äù is a negative term.
>    Final Answer: Opposes the policy.

---

## üîÅ CoT Inside ReAct: How It‚Äôs Used Together

CoT can be **nested inside ReAct‚Äôs `Thought:` steps**, giving structured reasoning before tool use.

```plaintext
Thought: First, I‚Äôll identify the topic of this document. Then I‚Äôll look for supporting evidence. Finally, I‚Äôll match it to a code.
Action: classify_document
Action Input: "Text..."
```

Used this way, CoT improves **traceability**, especially in qualitative and mixed-method workflows.

---

## üõ† How to Implement CoT as a Developer

### üîπ Prompt Template

```python
prompt = f"""
Question: {user_input}
Let‚Äôs think step by step.
"""
```

You can enhance it with:

* Few-shot examples
* Prepend phrases like:

  * ‚ÄúLet‚Äôs reason through this.‚Äù
  * ‚ÄúHere‚Äôs how I‚Äôll approach the problem.‚Äù

### üîπ Fine-Tuning

Many models (like `openchat`, `phi-2`, `deepseek`, `GPT-4o`) **respond well to CoT prompting out of the box** ‚Äî no fine-tuning needed.

If fine-tuning: train on `question + step-by-step + answer` pairs.

---

üîπ**Example: Qualitative Interview Coding (Social Science)**

**Research Prompt:**
*‚ÄúClassify the participant's statement into one of the following sentiment codes: `Supports`, `Neutral`, or `Opposes` regarding trust in government interventions.‚Äù*

**Quote:**

> *‚ÄúThey keep saying it‚Äôs for our safety, but I don‚Äôt feel like anyone‚Äôs actually listening to us.‚Äù*

---

### üß† CoT Response:

> 1. The speaker references the government's justification: ‚Äúit‚Äôs for our safety.‚Äù
> 2. However, this is followed by a personal sentiment: ‚ÄúI don‚Äôt feel like anyone‚Äôs actually listening.‚Äù
> 3. The contrast indicates skepticism or dissatisfaction.
>    Final Answer: **Opposes**

---

### ‚úÖ Why this is a good CoT use case:

* **Interpretive nuance** ‚Äî not just keyword-based
* **Clear intermediate reasoning** ‚Äî allows auditability and training
* **Applicable to automated coding agents** ‚Äî can be embedded in `Thought:` field of a ReAct loop or used alone

---

### üõ† Prompt Template You Can Use in Practice

```text
Classify the participant's statement into one of the following codes: Supports, Neutral, Opposes (regarding trust in government interventions).

Quote:
"They keep saying it‚Äôs for our safety, but I don‚Äôt feel like anyone‚Äôs actually listening to us."

Let‚Äôs think step by step.
```

This will trigger CoT behavior in GPT-4, Claude, Gemini 1.5, or even open models like Mistral, DeepSeek, or OpenChat.

---

## ‚úÖ When to Use CoT

| Use Case                       | Why CoT Helps                           |
| ------------------------------ | --------------------------------------- |
| Survey response interpretation | Makes reasoning explicit and verifiable |
| Policy stance classification   | Helps model explain judgment            |
| Arithmetic, logic, deduction   | Reduces hallucination                   |
| Coding open-ended responses    | Mirrors how researchers think aloud     |

---

## ‚öñÔ∏è CoT vs. ReAct vs. ToT

| Feature         | CoT               | ReAct                   | ToT                        |
| --------------- | ----------------- | ----------------------- | -------------------------- |
| Tools used      | ‚ùå No              | ‚úÖ Yes                   | ‚ùå (typically)              |
| Looping         | ‚ùå One-pass        | ‚úÖ Iterative             | ‚úÖ Exploratory              |
| Clarity         | ‚úÖ High            | ‚úÖ Medium                | ‚ùå Often abstract           |
| Deployment ease | ‚úÖ Easy            | ‚ö†Ô∏è Moderate             | üöß Complex                 |
| Best for        | Clear logic tasks | Tool-use, API workflows | Planning, open-ended paths |

---

## ‚ö†Ô∏è Limitations

* **No tool use**: Can‚Äôt fetch data, call APIs, or look up facts
* **No feedback loop**: Doesn‚Äôt adjust if wrong mid-stream
* **Overconfidence risk**: Can look logical even when incorrect
* **Doesn‚Äôt explore alternatives**: Only one reasoning path is followed

---

## üß† Summary

Use **Chain-of-Thought** when:

* You want **transparency** in logic
* Tasks don‚Äôt require external tools
* You‚Äôre working with **structured classification or deduction**

It‚Äôs the **default cognitive scaffold** inside many larger agent architectures ‚Äî often wrapped by ReAct for execution or ToT for exploration.

---
## Related Modules


| Module                   | Description                                                                                          |
| ------------------------ | ---------------------------------------------------------------------------------------------------- |
| `agent.md`    | Overview of LLM-based agents: what they are, how they differ from standalone models, and how reasoning, memory, and tools are orchestrated in modern agent architectures. Includes examples of agent workflows and social science use cases. |
| `agent_frameworks.md`    | Overview and classification of reasoning frameworks (CoT, ReAct, ToT) for agent design               |
| `cot.md`                 | Explanation of Chain-of-Thought (CoT) reasoning with breakdowns, examples, and visuals               |
| `react.md`               | ReAct framework loop explained: Thought ‚Üí Action ‚Üí Observation cycle and tool use                    |
| `tot.md`                 | Tree-of-Thoughts logic explained with examples of multi-path reasoning and selection                 |
| `tools.md`               | Agent tool registry and usage pattern (how to define, route, and call external tools)                |
| `extensions.md`          | How to extend agent capabilities with memory, RAG, chaining, or planning mechanisms                  |
| `function_call.md`       | OpenAI-style function calling interface: how LLMs call structured functions in agent workflows       |
| `targeted_learning.md`   | Prompt engineering strategies for social science use cases: guided logic, labels, and intent control |
| `langchain_basics.ipynb` | Jupyter notebook demo of basic agent setup using LangChain (tools, prompt templates, loop)           |
| `cot_vs_react_comparison.ipynb` | Side-by-side notebook comparing CoT-only vs ReAct-based workflows on social science tasks (e.g. coding, classification, search + reasoning). Includes performance notes, structure differences, and when to use which. |
| `cot_finetune.ipynb)` |  Notebook demo showing how to fine-tune a language model (e.g. Mistral, Phi-2) on Chain-of-Thought (CoT) reasoning patterns using instruction-style datasets. Includes PEFT-based LoRA setup, custom training examples for social science tasks, and explanation of when fine-tuning is necessary versus prompt-only CoT.|

---

## üîÆ What‚Äôs Next?

Next: [react.md]()

‚û°Ô∏è You‚Äôve explored linear reasoning with Chain-of-Thought ‚Äî now we shift to **ReAct**, where agents don‚Äôt just think, they act.
In this module, you‚Äôll learn how LLMs follow a **Thought ‚Üí Action ‚Üí Observation** loop to interact with real-world tools, adjust their reasoning based on feedback, and handle multi-step tasks.
This framework powers tool-using agents ‚Äî from document classification to dynamic search, summarization, and decision-making workflows in social science and beyond.
